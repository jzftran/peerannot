{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generates a large dataset for experiments using `dask`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install h5py 'dask[complete]'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dask.distributed import LocalCluster\n",
    "cluster = LocalCluster(memory_limit='500MB')\n",
    "client = cluster.get_client()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "Unable to synchronously create file (unable to truncate a file which is already open)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mOSError\u001b[39m                                   Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[13]\u001b[39m\u001b[32m, line 24\u001b[39m\n\u001b[32m     10\u001b[39m batch_size = \u001b[32m10000\u001b[39m\n\u001b[32m     11\u001b[39m \u001b[38;5;66;03m# chunks = (10, 10, 10)\u001b[39;00m\n\u001b[32m     12\u001b[39m \u001b[38;5;66;03m# rng = da.random.default_rng()\u001b[39;00m\n\u001b[32m     13\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m     20\u001b[39m \n\u001b[32m     21\u001b[39m \u001b[38;5;66;03m# s.to_hdf5(\"mytestfile.hdf5\", '/mydataset')\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m24\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43mh5py\u001b[49m\u001b[43m.\u001b[49m\u001b[43mFile\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmytestfile.hdf5\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mw\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[32m     25\u001b[39m     dset = f.create_dataset(\u001b[33m\"\u001b[39m\u001b[33mmydataset\u001b[39m\u001b[33m\"\u001b[39m, (dim1, dim2, dim3), dtype=\u001b[38;5;28mbool\u001b[39m)\n\u001b[32m     27\u001b[39m     \u001b[38;5;66;03m# Outer loop with progress bar\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/repos/peerannot/.venv/lib/python3.12/site-packages/h5py/_hl/files.py:564\u001b[39m, in \u001b[36mFile.__init__\u001b[39m\u001b[34m(self, name, mode, driver, libver, userblock_size, swmr, rdcc_nslots, rdcc_nbytes, rdcc_w0, track_order, fs_strategy, fs_persist, fs_threshold, fs_page_size, page_buf_size, min_meta_keep, min_raw_keep, locking, alignment_threshold, alignment_interval, meta_block_size, **kwds)\u001b[39m\n\u001b[32m    555\u001b[39m     fapl = make_fapl(driver, libver, rdcc_nslots, rdcc_nbytes, rdcc_w0,\n\u001b[32m    556\u001b[39m                      locking, page_buf_size, min_meta_keep, min_raw_keep,\n\u001b[32m    557\u001b[39m                      alignment_threshold=alignment_threshold,\n\u001b[32m    558\u001b[39m                      alignment_interval=alignment_interval,\n\u001b[32m    559\u001b[39m                      meta_block_size=meta_block_size,\n\u001b[32m    560\u001b[39m                      **kwds)\n\u001b[32m    561\u001b[39m     fcpl = make_fcpl(track_order=track_order, fs_strategy=fs_strategy,\n\u001b[32m    562\u001b[39m                      fs_persist=fs_persist, fs_threshold=fs_threshold,\n\u001b[32m    563\u001b[39m                      fs_page_size=fs_page_size)\n\u001b[32m--> \u001b[39m\u001b[32m564\u001b[39m     fid = \u001b[43mmake_fid\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muserblock_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfapl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfcpl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mswmr\u001b[49m\u001b[43m=\u001b[49m\u001b[43mswmr\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    566\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(libver, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[32m    567\u001b[39m     \u001b[38;5;28mself\u001b[39m._libver = libver\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/repos/peerannot/.venv/lib/python3.12/site-packages/h5py/_hl/files.py:244\u001b[39m, in \u001b[36mmake_fid\u001b[39m\u001b[34m(name, mode, userblock_size, fapl, fcpl, swmr)\u001b[39m\n\u001b[32m    242\u001b[39m     fid = h5f.create(name, h5f.ACC_EXCL, fapl=fapl, fcpl=fcpl)\n\u001b[32m    243\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m mode == \u001b[33m'\u001b[39m\u001b[33mw\u001b[39m\u001b[33m'\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m244\u001b[39m     fid = \u001b[43mh5f\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mh5f\u001b[49m\u001b[43m.\u001b[49m\u001b[43mACC_TRUNC\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfapl\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfapl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfcpl\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfcpl\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    245\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m mode == \u001b[33m'\u001b[39m\u001b[33ma\u001b[39m\u001b[33m'\u001b[39m:\n\u001b[32m    246\u001b[39m     \u001b[38;5;66;03m# Open in append mode (read/write).\u001b[39;00m\n\u001b[32m    247\u001b[39m     \u001b[38;5;66;03m# If that fails, create a new file only if it won't clobber an\u001b[39;00m\n\u001b[32m    248\u001b[39m     \u001b[38;5;66;03m# existing one (ACC_EXCL)\u001b[39;00m\n\u001b[32m    249\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mh5py/_objects.pyx:54\u001b[39m, in \u001b[36mh5py._objects.with_phil.wrapper\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mh5py/_objects.pyx:55\u001b[39m, in \u001b[36mh5py._objects.with_phil.wrapper\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mh5py/h5f.pyx:122\u001b[39m, in \u001b[36mh5py.h5f.create\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[31mOSError\u001b[39m: Unable to synchronously create file (unable to truncate a file which is already open)"
     ]
    }
   ],
   "source": [
    "# generate dataset ~100GB\n",
    "import h5py\n",
    "import numpy as np\n",
    "import dask.array as da\n",
    "from tqdm import tqdm\n",
    "import sparse\n",
    "dim1 = int(1e3)\n",
    "dim2 = int(1e2)\n",
    "dim3 = int(1e2)\n",
    "batch_size = 10000\n",
    "# chunks = (10, 10, 10)\n",
    "# rng = da.random.default_rng()\n",
    "\n",
    "# x = rng.random((dim1,dim2,dim3), chunks=chunks)\n",
    "\n",
    "# x[x<0.99] = 0\n",
    "# x[x !=0] = 1\n",
    "# s = x.map_blocks(sparse.COO, dtype=bool)\n",
    "\n",
    "\n",
    "# s.to_hdf5(\"mytestfile.hdf5\", '/mydataset')\n",
    "\n",
    "\n",
    "with h5py.File(\"mytestfile.hdf5\", \"w\") as f:\n",
    "    dset = f.create_dataset(\"mydataset\", (dim1, dim2, dim3), dtype=bool)\n",
    "\n",
    "    # Outer loop with progress bar\n",
    "    for i in tqdm(range(dim1), desc=\"Processing dim1\", unit=\"slice\"):\n",
    "        # Generate all random indices for this batch\n",
    "        rand_indices = np.random.randint(0, dim3, size=dim2)\n",
    "\n",
    "        # Inner batch processing with progress bar\n",
    "        for j_start in tqdm(range(0, dim2, batch_size),\n",
    "                          desc=f\"dim1={i}\",\n",
    "                          unit=\"batch\",\n",
    "                          leave=False):\n",
    "            j_end = min(j_start + batch_size, dim2)\n",
    "            batch_indices = rand_indices[j_start:j_end]\n",
    "\n",
    "            # Create boolean array for this batch\n",
    "            batch = np.zeros((j_end-j_start, dim3), dtype=bool)\n",
    "            batch[np.arange(j_end-j_start), batch_indices] = True\n",
    "\n",
    "            # Write the batch\n",
    "            dset[i, j_start:j_end] = batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-04-10 23:47:29.346\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mpeerannot.models.aggregation.DS\u001b[0m:\u001b[36m_init_crowd_matrix\u001b[0m:\u001b[36m107\u001b[0m - \u001b[34m\u001b[1mSize of dense crowd matrix: 10144\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "627031f5463d4ade956e397d24249d85",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Dawid and Skene:   0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "([dask.array<log, shape=(), dtype=float64, chunksize=(), chunktype=numpy.ndarray>,\n",
       "  dask.array<log, shape=(), dtype=float64, chunksize=(), chunktype=numpy.ndarray>,\n",
       "  dask.array<log, shape=(), dtype=float64, chunksize=(), chunktype=numpy.ndarray>,\n",
       "  dask.array<log, shape=(), dtype=float64, chunksize=(), chunktype=numpy.ndarray>,\n",
       "  dask.array<log, shape=(), dtype=float64, chunksize=(), chunktype=numpy.ndarray>],\n",
       " 5)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import warnings\n",
    "from collections.abc import Generator\n",
    "from os import PathLike\n",
    "from sys import getsizeof\n",
    "from typing import Annotated\n",
    "\n",
    "import numpy as np\n",
    "from annotated_types import Ge\n",
    "from loguru import logger\n",
    "from numpy.typing import NDArray\n",
    "from pydantic import validate_call\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "from peerannot.models.aggregation.warnings import DidNotConverge\n",
    "from peerannot.models.aggregation.dawid_skene import DawidSkene\n",
    "from peerannot.models.template import AnswersDict, CrowdModel\n",
    "import sparse as sp\n",
    "FilePathInput = PathLike | str | list[str] | Generator[str, None, None] | None\n",
    "\n",
    "\n",
    "class DaskDawidSkene(DawidSkene):\n",
    "    \"\"\"\n",
    "    =============================\n",
    "    Dawid and Skene model (1979)\n",
    "    =============================\n",
    "\n",
    "    Assumptions:\n",
    "    - independent workers\n",
    "\n",
    "    Using:\n",
    "    - EM algorithm\n",
    "\n",
    "    Estimating:\n",
    "    - One confusion matrix for each workers\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "\n",
    "    def _init_T(self) -> None:  # noqa: N802\n",
    "        \"\"\"NS initialization\"\"\"\n",
    "        # T shape is n_task, n_classes\n",
    "        T = self.crowd_matrix.sum(axis=1)  # noqa: N806\n",
    "\n",
    "        tdim = T.sum(1, keepdims=True)\n",
    "        self.T = da.where(tdim > 0, T / tdim, 0)\n",
    "\n",
    "\n",
    "    def _m_step(\n",
    "        self,\n",
    "    ) -> None:\n",
    "        \"\"\"Maximizing log likelihood (see eq. 2.3 and 2.4 Dawid and Skene 1979)\n",
    "\n",
    "        Returns:\n",
    "            :math:`\\\\rho`: :math:`(\\\\rho_j)_j` probabilities that instance has\n",
    "                true response j if drawn at random (class marginals)\n",
    "            pi: number of times worker k records l when j is correct\n",
    "        \"\"\"\n",
    "        \n",
    "        self.rho = self.T.sum(0) / self.n_task\n",
    "        pi = da.einsum('tq,twc->wqc', self.T, self.crowd_matrix)\n",
    "        denom = pi.sum(axis=2, keepdims=True)\n",
    "        self.pi = pi / da.where(denom <= 0, -1e9, denom)\n",
    "\n",
    "\n",
    "    def _e_step(self) -> None:\n",
    "        \"\"\"Estimate indicator variables (see eq. 2.5 Dawid and Skene 1979)\"\"\"\n",
    "\n",
    "        exp_pi = da.power(self.pi[da.newaxis, :, :, :], self.crowd_matrix[:, :, da.newaxis, :])\n",
    "\n",
    "        # numerator by taking the product over the worker axis\n",
    "        num = da.prod(exp_pi, axis=3).prod(axis=1) * self.rho[da.newaxis, :]\n",
    "        self.denom_e_step = num.sum(axis=1, keepdims=True)\n",
    "        self.T = da.where(self.denom_e_step > 0, num / self.denom_e_step, num)\n",
    "\n",
    "\n",
    "\n",
    "    def _log_likelihood(self) -> float:\n",
    "        \"\"\"Compute log likelihood of the model\"\"\"\n",
    "        return da.log(da.sum(self.denom_e_step))\n",
    "\n",
    "    @validate_call\n",
    "    def run(\n",
    "        self,\n",
    "        epsilon: Annotated[float, Ge(0)] = 1e-6,\n",
    "        maxiter: Annotated[int, Ge(0)] = 50,\n",
    "    ) -> tuple[list[float], int]:\n",
    "        \"\"\"Run the EM optimization\n",
    "\n",
    "        :param epsilon: stopping criterion (:math:`\\\\ell_1` norm between two iterates of log likelihood), defaults to 1e-6\n",
    "        :type epsilon: float, optional\n",
    "        :param maxiter: Maximum number of steps, defaults to 50\n",
    "        :type maxiter: int, optional\n",
    "        :param verbose: Verbosity level, defaults to False\n",
    "        :return: Log likelihood values and number of steps taken\n",
    "        :rtype: (list,int)\n",
    "        \"\"\"\n",
    "\n",
    "        i = 0\n",
    "        eps = np.inf\n",
    "\n",
    "        self._init_T()\n",
    "        ll = []\n",
    "        pbar = tqdm(total=maxiter, desc=\"Dawid and Skene\")\n",
    "        while i < maxiter and eps > epsilon:\n",
    "            self._m_step()\n",
    "            self._e_step()\n",
    "            likeli = self._log_likelihood()\n",
    "            ll.append(likeli)\n",
    "            if i > 0:\n",
    "                eps = da.abs(ll[-1] - ll[-2])\n",
    "            i += 1\n",
    "            pbar.update(1)\n",
    "\n",
    "        pbar.set_description(\"Finished\")\n",
    "        pbar.close()\n",
    "        self.c = i\n",
    "        if eps > epsilon:\n",
    "            warnings.warn(\n",
    "                DidNotConverge(self.__class__.__name__, eps, epsilon),\n",
    "                stacklevel=2,\n",
    "            )\n",
    "\n",
    "        return ll, i\n",
    "\n",
    "    def get_answers(self) -> NDArray:\n",
    "        \"\"\"Get most probable labels\"\"\"\n",
    "\n",
    "        return np.vectorize(self.inv_labels.get)(\n",
    "            np.argmax(self.get_probas(), axis=1),\n",
    "        )\n",
    "\n",
    "    def get_probas(self) -> NDArray:\n",
    "        \"\"\"Get soft labels distribution for each task\"\"\"\n",
    "        return self.T\n",
    "    \n",
    "\n",
    "from peerannot.models import DawidSkene\n",
    "from types import MethodType\n",
    "import dask.array as da\n",
    "import h5py\n",
    "import sparse\n",
    "f = h5py.File(\"mytestfile.hdf5\", \"r\")\n",
    "dset = f[\"mydataset\"]\n",
    "\n",
    "dense_test_crowd_matrix = da.from_array(dset, chunks=(1000,1000,1000))\n",
    "test_crowd_matrix = dense_test_crowd_matrix.map_blocks(sparse.COO)\n",
    "\n",
    "\n",
    "dds = DaskDawidSkene.from_crowd_matrix(test_crowd_matrix)\n",
    "dds.run(maxiter=50)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "    <tr>\n",
       "        <td>\n",
       "            <table style=\"border-collapse: collapse;\">\n",
       "                <thead>\n",
       "                    <tr>\n",
       "                        <td> </td>\n",
       "                        <th> Array </th>\n",
       "                        <th> Chunk </th>\n",
       "                    </tr>\n",
       "                </thead>\n",
       "                <tbody>\n",
       "                    \n",
       "                    <tr>\n",
       "                        <th> Shape </th>\n",
       "                        <td> (100,) </td>\n",
       "                        <td> (100,) </td>\n",
       "                    </tr>\n",
       "                    <tr>\n",
       "                        <th> Dask graph </th>\n",
       "                        <td colspan=\"2\"> 1 chunks in 132 graph layers </td>\n",
       "                    </tr>\n",
       "                    <tr>\n",
       "                        <th> Data type </th>\n",
       "                        <td colspan=\"2\"> float64 sparse._coo.core.COO </td>\n",
       "                    </tr>\n",
       "                </tbody>\n",
       "            </table>\n",
       "        </td>\n",
       "        <td>\n",
       "        <svg width=\"170\" height=\"75\" style=\"stroke:rgb(0,0,0);stroke-width:1\" >\n",
       "\n",
       "  <!-- Horizontal lines -->\n",
       "  <line x1=\"0\" y1=\"0\" x2=\"120\" y2=\"0\" style=\"stroke-width:2\" />\n",
       "  <line x1=\"0\" y1=\"25\" x2=\"120\" y2=\"25\" style=\"stroke-width:2\" />\n",
       "\n",
       "  <!-- Vertical lines -->\n",
       "  <line x1=\"0\" y1=\"0\" x2=\"0\" y2=\"25\" style=\"stroke-width:2\" />\n",
       "  <line x1=\"120\" y1=\"0\" x2=\"120\" y2=\"25\" style=\"stroke-width:2\" />\n",
       "\n",
       "  <!-- Colored Rectangle -->\n",
       "  <polygon points=\"0.0,0.0 120.0,0.0 120.0,25.412616514582485 0.0,25.412616514582485\" style=\"fill:#ECB172A0;stroke-width:0\"/>\n",
       "\n",
       "  <!-- Text -->\n",
       "  <text x=\"60.000000\" y=\"45.412617\" font-size=\"1.0rem\" font-weight=\"100\" text-anchor=\"middle\" >100</text>\n",
       "  <text x=\"140.000000\" y=\"12.706308\" font-size=\"1.0rem\" font-weight=\"100\" text-anchor=\"middle\" transform=\"rotate(0,140.000000,12.706308)\">1</text>\n",
       "</svg>\n",
       "        </td>\n",
       "    </tr>\n",
       "</table>"
      ],
      "text/plain": [
       "dask.array<getitem, shape=(100,), dtype=float64, chunksize=(100,), chunktype=sparse.COO>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dds.T[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 100, 100)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_crowd_matrix.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
