{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generates a large dataset for experiments using `dask`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install h5py dask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing dim1: 100%|██████████| 10/10 [02:17<00:00, 13.72s/slice]\n"
     ]
    }
   ],
   "source": [
    "# generate dataset ~100GB\n",
    "import h5py\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "dim1 = int(1e1)\n",
    "dim2 = int(1e6)\n",
    "dim3 = int(1e4)\n",
    "batch_size = 10000\n",
    "\n",
    "with h5py.File(\"mytestfile.hdf5\", \"w\") as f:\n",
    "    dset = f.create_dataset(\"mydataset\", (dim1, dim2, dim3), dtype=bool)\n",
    "\n",
    "    # Outer loop with progress bar\n",
    "    for i in tqdm(range(dim1), desc=\"Processing dim1\", unit=\"slice\"):\n",
    "        # Generate all random indices for this batch\n",
    "        rand_indices = np.random.randint(0, dim3, size=dim2)\n",
    "\n",
    "        # Inner batch processing with progress bar\n",
    "        for j_start in tqdm(range(0, dim2, batch_size),\n",
    "                          desc=f\"dim1={i}\",\n",
    "                          unit=\"batch\",\n",
    "                          leave=False):\n",
    "            j_end = min(j_start + batch_size, dim2)\n",
    "            batch_indices = rand_indices[j_start:j_end]\n",
    "\n",
    "            # Create boolean array for this batch\n",
    "            batch = np.zeros((j_end-j_start, dim3), dtype=bool)\n",
    "            batch[np.arange(j_end-j_start), batch_indices] = True\n",
    "\n",
    "            # Write the batch\n",
    "            dset[i, j_start:j_end] = batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask.array as da\n",
    "import h5py\n",
    "\n",
    "with h5py.File(\"mytestfile.hdf5\", \"r\") as f:\n",
    "    dset = f[\"mydataset\"]\n",
    "\n",
    "    test_crowd_matrix = da.from_array(dset, chunks=(10,10_000,10_000))\n",
    "\n",
    "\n",
    "    T = test_crowd_matrix.sum(axis=1)\n",
    "\n",
    "    tdim = T.sum(1, keepdims=True)\n",
    "    T = da.where(tdim > 0, T / tdim, 0).compute()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_crowd_matrix sparsity\n",
    "import dask.array as da\n",
    "import h5py\n",
    "\n",
    "with h5py.File(\"mytestfile.hdf5\", \"r\") as f:\n",
    "    dset = f[\"mydataset\"]\n",
    "\n",
    "    test_crowd_matrix = da.from_array(dset, chunks=(10,1_000,1_000))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    sparsity = 1-(da.count_nonzero(test_crowd_matrix)/ test_crowd_matrix.size).compute()\n",
    "    print(sparsity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jozef/Desktop/repos/peerannot/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "from collections.abc import Generator\n",
    "from os import PathLike\n",
    "from sys import getsizeof\n",
    "from typing import Annotated\n",
    "\n",
    "import numpy as np\n",
    "from annotated_types import Ge\n",
    "from loguru import logger\n",
    "from numpy.typing import NDArray\n",
    "from pydantic import validate_call\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "from peerannot.models.aggregation.warnings import DidNotConverge\n",
    "from peerannot.models.template import AnswersDict, CrowdModel\n",
    "import sparse as sp\n",
    "FilePathInput = PathLike | str | list[str] | Generator[str, None, None] | None\n",
    "\n",
    "\n",
    "class DaskDawidSkene(CrowdModel):\n",
    "    \"\"\"\n",
    "    =============================\n",
    "    Dawid and Skene model (1979)\n",
    "    =============================\n",
    "\n",
    "    Assumptions:\n",
    "    - independent workers\n",
    "\n",
    "    Using:\n",
    "    - EM algorithm\n",
    "\n",
    "    Estimating:\n",
    "    - One confusion matrix for each workers\n",
    "    \"\"\"\n",
    "\n",
    "    @validate_call\n",
    "    def __init__(\n",
    "        self,\n",
    "        n_task: int,\n",
    "        n_workers: Annotated[int, Ge(1)],\n",
    "        n_classes: Annotated[int, Ge(1)],\n",
    "            ) -> None:\n",
    "        r\"\"\"Dawid and Skene strategy: estimate confusion matrix for each worker.\n",
    "\n",
    "        Assuming that workers are independent, the model assumes that\n",
    "\n",
    "        .. math::\n",
    "\n",
    "            (y_i^{(j)}\\ | y_i^\\\\star = k) \\\\sim \\\\mathcal{M}\\\\left(\\\\pi^{(j)}_{k,\\\\cdot}\\\\right)\n",
    "\n",
    "        and maximizes the log likelihood of the model using an EM algorithm.\n",
    "\n",
    "        .. math::\n",
    "\n",
    "            \\\\underset{\\\\rho,\\\\\\pi,T}{\\mathrm{argmax}}\\\\prod_{i\\\\in [n_{\\\\texttt{task}}]}\\prod_{k \\\\in [K]}\\\\bigg[\\\\rho_k\\prod_{j\\\\in [n_{\\\\texttt{worker}}]}\\prod_{\\\\ell\\in [K]}\\\\big(\\\\pi^{(j)}_{k, \\\\ell}\\\\big)^{\\mathbf{1}_{\\\\{y_i^{(j)}=\\\\ell\\\\}}}\\\\bigg]^{T_{i,k}},\n",
    "\n",
    "        where :math:`\\\\rho` is the class marginals, :math:`\\\\pi` is the confusion matrix and :math:`T` is the indicator variables of belonging to each class.\n",
    "\n",
    "        :param answers: Dictionary of workers answers with format\n",
    "\n",
    "         .. code-block:: javascript\n",
    "\n",
    "            {\n",
    "                task0: {worker0: label, worker1: label},\n",
    "                task1: {worker1: label}\n",
    "            }\n",
    "\n",
    "        :type answers: dict\n",
    "        :param sparse: If the number of workers/tasks/label is large (:math:`>10^{6}` for at least one), # use sparse=True to run per task\n",
    "        :param n_classes: Number of possible classes, defaults to 2\n",
    "        :type n_classes: int, optional\"\"\"\n",
    "\n",
    "        self.n_task:int = n_task\n",
    "        self.n_workers: int = n_workers\n",
    "        self.n_classes: int = n_classes\n",
    "        \n",
    "\n",
    "        \n",
    "    def _exclude_answers(self) -> None:\n",
    "        answers_modif = {}\n",
    "        if self.path_remove is not None:\n",
    "            to_remove = np.loadtxt(self.path_remove, dtype=int)\n",
    "            i = 0\n",
    "            for key, val in self.answers.items():\n",
    "                if int(key) not in to_remove[:, 1]:\n",
    "                    answers_modif[i] = val\n",
    "                    i += 1\n",
    "            self.answers = answers_modif\n",
    "\n",
    "    def _init_crowd_matrix(self,crowd_matrix) -> None:\n",
    "        \"\"\"Transform dictionnary of labels to a tensor of size\n",
    "        (n_task, n_workers, n_classes).\"\"\"\n",
    "\n",
    "        self.crowd_matrix = crowd_matrix\n",
    "\n",
    "    def _init_T(self) -> None:  # noqa: N802\n",
    "        \"\"\"NS initialization\"\"\"\n",
    "        # T shape is n_task, n_classes\n",
    "        T = self.crowd_matrix.sum(axis=1)  # noqa: N806\n",
    "\n",
    "        tdim = T.sum(1, keepdims=True)\n",
    "        self.T = da.where(tdim > 0, T / tdim, 0)\n",
    "\n",
    "    def _m_step(\n",
    "        self,\n",
    "    ) -> None:\n",
    "        \"\"\"Maximizing log likelihood (see eq. 2.3 and 2.4 Dawid and Skene 1979)\n",
    "\n",
    "        Returns:\n",
    "            :math:`\\\\rho`: :math:`(\\\\rho_j)_j` probabilities that instance has\n",
    "                true response j if drawn at random (class marginals)\n",
    "            pi: number of times worker k records l when j is correct\n",
    "        \"\"\"\n",
    "        rho = self.T.sum(0) / self.n_task\n",
    "\n",
    "        pi = da.zeros((self.n_workers, self.n_classes, self.n_classes), chunks=(10, 1000, 1000))\n",
    "        for q in range(self.n_classes):\n",
    "            pij = self.T[:, q] @ self.crowd_matrix.transpose((1, 0, 2))\n",
    "            denom = pij.sum(1)\n",
    "            pi[:, q, :] = pij / np.where(denom <= 0, -1e9, denom).reshape(\n",
    "                -1,\n",
    "                1,\n",
    "            )\n",
    "        self.rho, self.pi = rho, pi\n",
    "\n",
    "    def _e_step(self) -> None:\n",
    "        \"\"\"Estimate indicator variables (see eq. 2.5 Dawid and Skene 1979)\n",
    "\n",
    "        Returns:\n",
    "            T: New estimate for indicator variables (n_task, n_worker)\n",
    "            denom: value used to compute likelihood easily\n",
    "        \"\"\"\n",
    "        T = np.zeros((self.n_task, self.n_classes))  # noqa: N806\n",
    "        for i in range(self.n_task):\n",
    "            for j in range(self.n_classes):\n",
    "                num = (\n",
    "                    np.prod(\n",
    "                        np.power(self.pi[:, j, :], self.crowd_matrix[i, :, :]),\n",
    "                    )\n",
    "                    * self.rho[j]\n",
    "                )\n",
    "                T[i, j] = num\n",
    "        self.denom_e_step = T.sum(1, keepdims=True)\n",
    "        T = np.where(self.denom_e_step > 0, T / self.denom_e_step, T)  # noqa: N806\n",
    "        self.T = T\n",
    "\n",
    "\n",
    "    def _log_likelihood(self) -> float:\n",
    "        \"\"\"Compute log likelihood of the model\"\"\"\n",
    "        return np.log(np.sum(self.denom_e_step))\n",
    "\n",
    "    @validate_call\n",
    "    def run(\n",
    "        self,\n",
    "        epsilon: Annotated[float, Ge(0)] = 1e-6,\n",
    "        maxiter: Annotated[int, Ge(0)] = 50,\n",
    "    ) -> tuple[list[float], int]:\n",
    "        \"\"\"Run the EM optimization\n",
    "\n",
    "        :param epsilon: stopping criterion (:math:`\\\\ell_1` norm between two iterates of log likelihood), defaults to 1e-6\n",
    "        :type epsilon: float, optional\n",
    "        :param maxiter: Maximum number of steps, defaults to 50\n",
    "        :type maxiter: int, optional\n",
    "        :param verbose: Verbosity level, defaults to False\n",
    "        :return: Log likelihood values and number of steps taken\n",
    "        :rtype: (list,int)\n",
    "        \"\"\"\n",
    "\n",
    "        i = 0\n",
    "        eps = np.inf\n",
    "\n",
    "        self._init_T()\n",
    "        ll = []\n",
    "        pbar = tqdm(total=maxiter, desc=\"Dawid and Skene\")\n",
    "        while i < maxiter and eps > epsilon:\n",
    "            self._m_step()\n",
    "            self._e_step()\n",
    "            likeli = self._log_likelihood()\n",
    "            ll.append(likeli)\n",
    "            if i > 0:\n",
    "                eps = np.abs(ll[-1] - ll[-2])\n",
    "            i += 1\n",
    "            pbar.update(1)\n",
    "\n",
    "        pbar.set_description(\"Finished\")\n",
    "        pbar.close()\n",
    "        self.c = i\n",
    "        if eps > epsilon:\n",
    "            warnings.warn(\n",
    "                DidNotConverge(self.__class__.__name__, eps, epsilon),\n",
    "                stacklevel=2,\n",
    "            )\n",
    "\n",
    "        return ll, i\n",
    "\n",
    "    def get_answers(self) -> NDArray:\n",
    "        \"\"\"Get most probable labels\"\"\"\n",
    "\n",
    "        return np.vectorize(self.inv_labels.get)(\n",
    "            np.argmax(self.get_probas(), axis=1),\n",
    "        )\n",
    "\n",
    "    def get_probas(self) -> NDArray:\n",
    "        \"\"\"Get soft labels distribution for each task\"\"\"\n",
    "        return self.T\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Dawid and Skene:   0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "from peerannot.models import DawidSkene\n",
    "from types import MethodType\n",
    "import dask.array as da\n",
    "import h5py\n",
    "\n",
    "with h5py.File(\"mytestfile.hdf5\", \"r\") as f:\n",
    "    dset = f[\"mydataset\"]\n",
    "\n",
    "    test_crowd_matrix = da.from_array(dset, chunks=(10,1_000,1_000))\n",
    "\n",
    "\n",
    "\n",
    "    dds = DaskDawidSkene(\n",
    "        n_task = test_crowd_matrix.shape[0],\n",
    "        n_workers=test_crowd_matrix.shape[1],\n",
    "        n_classes=test_crowd_matrix.shape[2],\n",
    "    )\n",
    "    dds._init_crowd_matrix(test_crowd_matrix)\n",
    "    dds.run(maxiter=1)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
