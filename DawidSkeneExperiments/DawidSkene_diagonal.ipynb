{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dawid-Skene Algorithm using the diagonal of the confucion matrix\n",
    "\n",
    "if we use the diagonal of the confusion matrix $\\pi$ we assume:\n",
    "- annotators only have a probabilty of labeling the correct class correctly\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import super small toy dataset\n",
    "from simple_toy_data import votes as svotes, N_WORKERS as sN_WORKERS, N_CLASSES as sN_CLASSES\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "# those are equivalent\n",
    "pij = np.einsum(\"j,jil->il\", dsd.T[:,1], dsd.crowd_matrix) \n",
    "pij = dsd.T[:, 1] @ dsd.crowd_matrix.transpose((1, 0, 2))\n",
    "# then denomiator:\n",
    "denom = np.einsum(\"j,jil->i\", dsd.T[:,1], dsd.crowd_matrix) \n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jozef/Desktop/repos/peerannot/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "from os import PathLike\n",
    "from sys import getsizeof\n",
    "from typing import Annotated, Generator\n",
    "\n",
    "import numpy as np\n",
    "import sparse as sp\n",
    "from annotated_types import Ge\n",
    "from loguru import logger\n",
    "from memory_profiler import profile\n",
    "from pydantic import validate_call\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "from peerannot.models.template import CrowdModel,AnswersDict\n",
    "from peerannot.models import DawidSkene\n",
    "# load data\n",
    "from toy_data import votes, N_CLASSES, N_WORKERS\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "FilePathInput = PathLike | str | list[str] | Generator[str, None, None] | None\n",
    "\n",
    "\n",
    "class DawidSkeneDiagonal(CrowdModel):\n",
    "    \"\"\"\n",
    "    =============================\n",
    "    Dawid and Skene model (1979)\n",
    "    =============================\n",
    "\n",
    "    Assumptions:\n",
    "    - independent workers\n",
    "\n",
    "    Using:\n",
    "    - EM algorithm\n",
    "\n",
    "    Estimating:\n",
    "    - One confusion matrix for each workers\n",
    "    \"\"\"\n",
    "\n",
    "    @validate_call\n",
    "    def __init__(\n",
    "        self,\n",
    "        answers: AnswersDict,\n",
    "        # TODO@jzftran probably annotation with 0 or 1 worker doesn't make sense: for 0 it should be an error\n",
    "        n_workers: Annotated[int, Ge(1)],\n",
    "        n_classes: Annotated[int, Ge(1)],\n",
    "        *,\n",
    "        sparse: bool = False,\n",
    "        path_remove: FilePathInput = None,\n",
    "    ) -> None:\n",
    "        r\"\"\"Dawid and Skene strategy: estimate confusion matrix for each worker.\n",
    "\n",
    "        Assuming that workers are independent, the model assumes that\n",
    "\n",
    "        .. math::\n",
    "\n",
    "            (y_i^{(j)}\\ | y_i^\\\\star = k) \\\\sim \\\\mathcal{M}\\\\left(\\\\pi^{(j)}_{k,\\\\cdot}\\\\right)\n",
    "\n",
    "        and maximizes the log likelihood of the model using an EM algorithm.\n",
    "\n",
    "        .. math::\n",
    "\n",
    "            \\\\underset{\\\\rho,\\\\\\pi,T}{\\mathrm{argmax}}\\\\prod_{i\\\\in [n_{\\\\texttt{task}}]}\\prod_{k \\\\in [K]}\\\\bigg[\\\\rho_k\\prod_{j\\\\in [n_{\\\\texttt{worker}}]}\\prod_{\\\\ell\\in [K]}\\\\big(\\\\pi^{(j)}_{k, \\\\ell}\\\\big)^{\\mathbf{1}_{\\\\{y_i^{(j)}=\\\\ell\\\\}}}\\\\bigg]^{T_{i,k}},\n",
    "\n",
    "        where :math:`\\\\rho` is the class marginals, :math:`\\\\pi` is the confusion matrix and :math:`T` is the indicator variables of belonging to each class.\n",
    "\n",
    "        :param answers: Dictionary of workers answers with format\n",
    "\n",
    "         .. code-block:: javascript\n",
    "\n",
    "            {\n",
    "                task0: {worker0: label, worker1: label},\n",
    "                task1: {worker1: label}\n",
    "            }\n",
    "\n",
    "        :type answers: dict\n",
    "        :param sparse: If the number of workers/tasks/label is large (:math:`>10^{6}` for at least one), use sparse=True to run per task\n",
    "        :type sparse: bool, optional\n",
    "        :param n_classes: Number of possible classes, defaults to 2\n",
    "        :type n_classes: int, optional\"\"\"\n",
    "\n",
    "        super().__init__(answers)\n",
    "        self.n_workers: int = n_workers\n",
    "        self.n_classes: int = n_classes\n",
    "        self.sparse: bool = sparse\n",
    "        self.path_remove: FilePathInput = path_remove\n",
    "        self.n_task: int = len(self.answers)\n",
    "\n",
    "        self.exclude_answers()\n",
    "        if self.sparse:\n",
    "            raise NotImplementedError('The sparse algorithm for DawidSkeneDiagonal in the current context is not implemented.')\n",
    "        else:\n",
    "            self.init_crowd_matrix()\n",
    "\n",
    "    def exclude_answers(self) -> None:\n",
    "        answers_modif = {}\n",
    "        if self.path_remove is not None:\n",
    "            to_remove = np.loadtxt(self.path_remove, dtype=int)\n",
    "            i = 0\n",
    "            for key, val in self.answers.items():\n",
    "                if int(key) not in to_remove[:, 1]:\n",
    "                    answers_modif[i] = val\n",
    "                    i += 1\n",
    "            self.answers = answers_modif\n",
    "\n",
    "\n",
    "    def init_crowd_matrix(self)->None:\n",
    "        \"\"\"Transform dictionnary of labels to a tensor of size (n_task, n_workers, n_classes).\"\"\"\n",
    "\n",
    "        matrix = np.zeros((self.n_task, self.n_workers, self.n_classes))\n",
    "        for task, ans in self.answers.items():\n",
    "            for worker, label in ans.items():\n",
    "                matrix[task, worker, label] += 1\n",
    "\n",
    "        logger.debug(f\"Dense crowd matrix  {getsizeof(matrix)}\")\n",
    "        # shape\n",
    "        self.crowd_matrix = matrix\n",
    "\n",
    "    def init_T(self):\n",
    "        \"\"\"NS initialization\"\"\"\n",
    "        # shape (n_task, n_class)\n",
    "        T = self.crowd_matrix.sum(axis=1)\n",
    "        logger.debug(f\"Size of T before calc: {getsizeof(T)}\")\n",
    "\n",
    "        tdim = T.sum(1, keepdims=True)\n",
    "        self.T = np.where(tdim > 0, T / tdim, 0)\n",
    "        logger.debug(f\"Size of T: {getsizeof(self.T)}\")\n",
    "\n",
    "\n",
    "\n",
    "    def _m_step(self) -> None:\n",
    "        \"\"\"Maximizing log likelihood with only diagonal elements of pi.\"\"\"\n",
    "        rho = self.T.sum(0) / self.n_task\n",
    "\n",
    "        pi = np.zeros((self.n_workers, self.n_classes))\n",
    "        for j in range(self.n_classes):\n",
    "            # TODO:@jzftran change this to calculate diagonal faster\n",
    "            pij = self.T[:, j] @ self.crowd_matrix.transpose((1, 0, 2))\n",
    "            denom = pij.sum(1)\n",
    "\n",
    "\n",
    "            diag_values = pij[:, j] / np.where(denom > 0, denom, 1e-9)\n",
    "            pi[:, j] = diag_values\n",
    "\n",
    "\n",
    "        # pi shape (n_workers, n_class), reresents how sure worker is sure that the label j is true\n",
    "        pi_non_diag_values = (np.ones_like(pi) - pi) / (self.n_classes - 1)\n",
    "        self.rho, self.pi, self.pi_non_diag_values = rho, pi, pi_non_diag_values\n",
    "\n",
    "\n",
    "    def _e_step(self) -> None:\n",
    "        \"\"\"Estimate indicator variables (see eq. 2.5 Dawid and Skene 1979)\n",
    "\n",
    "        Returns:\n",
    "            T: New estimate for indicator variables (n_task, n_worker)\n",
    "            denom: value used to compute likelihood easily\n",
    "        \"\"\"\n",
    "        T = np.zeros((self.n_task, self.n_classes))\n",
    "\n",
    "        worker_probs = np.zeros(self.n_workers)\n",
    "        \n",
    "        for i in range(self.n_task):\n",
    "            for j in range(self.n_classes):\n",
    "                # Compute per-worker probabilities\n",
    "                for k in range(self.n_workers):\n",
    "                    # Worker k's annotations for task i\n",
    "                    worker_labels = self.crowd_matrix[i, k, :]  # shape (n_classes,)\n",
    "                    \n",
    "                    # Extract the diagonal and off-diagonal contributions\n",
    "                    diag_contrib = np.power( self.pi[k, j], worker_labels[j])  # When label j is actually assigned\n",
    "                    off_diag_contrib = np.prod(\n",
    "                        np.power(self.pi_non_diag_values[k, j] , worker_labels[np.arange(self.n_classes) != j])\n",
    "                    )\n",
    "\n",
    "                    worker_probs[k] = diag_contrib * off_diag_contrib\n",
    "\n",
    "                num = np.prod(worker_probs) * self.rho[j]\n",
    "                T[i, j] = num\n",
    "\n",
    "        self.denom_e_step = T.sum(1, keepdims=True)\n",
    "        T = np.where(self.denom_e_step > 0, T / self.denom_e_step, T)\n",
    "        self.T = T\n",
    "\n",
    "\n",
    "    def log_likelihood(self):\n",
    "        \"\"\"Compute log likelihood of the model\"\"\"\n",
    "        return np.log(np.sum(self.denom_e_step))\n",
    "\n",
    "    def run_dense(\n",
    "        self,\n",
    "        epsilon: Annotated[float, Ge(0)] = 1e-6,\n",
    "        maxiter: Annotated[int, Ge(0)] = 50,\n",
    "        *,\n",
    "        verbose: bool = False,\n",
    "    ) -> tuple[list[np.float64], int]:\n",
    "        i = 0\n",
    "        eps = np.inf\n",
    "\n",
    "        self.init_T()\n",
    "        ll = []\n",
    "        pbar = tqdm(total=maxiter, desc=\"Dawid and Skene\")\n",
    "        while i < maxiter and eps > epsilon:\n",
    "            self._m_step()\n",
    "            self._e_step()\n",
    "            likeli = self.log_likelihood()\n",
    "            ll.append(likeli)\n",
    "            if len(ll) >= 2:\n",
    "                eps = np.abs(ll[-1] - ll[-2])\n",
    "            i += 1\n",
    "            pbar.update(1)\n",
    "\n",
    "        pbar.set_description(\"Finished\")\n",
    "        pbar.close()\n",
    "        self.c = i\n",
    "        if eps > epsilon and verbose:\n",
    "            print(f\"DS did not converge: err={eps}\")\n",
    "        return ll, i\n",
    "\n",
    "\n",
    "    @validate_call\n",
    "    def run(\n",
    "        self,\n",
    "        epsilon: Annotated[float, Ge(0)] = 1e-6,\n",
    "        maxiter: Annotated[int, Ge(0)] = 50,\n",
    "        *,\n",
    "        verbose: bool = False,\n",
    "    ) -> tuple[list[np.float64], int]:\n",
    "        \"\"\"Run the EM optimization\n",
    "\n",
    "        :param epsilon: stopping criterion (:math:`\\\\ell_1` norm between two iterates of log likelihood), defaults to 1e-6\n",
    "        :type epsilon: float, optional\n",
    "        :param maxiter: Maximum number of steps, defaults to 50\n",
    "        :type maxiter: int, optional\n",
    "        :param verbose: Verbosity level, defaults to False\n",
    "        :type verbose: bool, optional\n",
    "        :return: Log likelihood values and number of steps taken\n",
    "        :rtype: (list,int)\n",
    "        \"\"\"\n",
    "\n",
    "        if self.sparse:\n",
    "            raise NotImplementedError('The sparse algorithm for DawidSkeneShared in the current context is not implemented.')\n",
    "        return self.run_dense(\n",
    "            epsilon=epsilon,\n",
    "            maxiter=maxiter,\n",
    "            verbose=verbose,\n",
    "        )\n",
    "\n",
    "    def get_answers(self):\n",
    "        \"\"\"Get most probable labels\"\"\"\n",
    "        if self.sparse:\n",
    "            return np.vectorize(self.inv_labels.get)(\n",
    "                sp.argmax(self.sparse_T, axis=1).todense()\n",
    "            )\n",
    "        return np.vectorize(self.inv_labels.get)(\n",
    "            np.argmax(self.get_probas(), axis=1)\n",
    "        )\n",
    "\n",
    "    def get_probas(self):\n",
    "        \"\"\"Get soft labels distribution for each task\"\"\"\n",
    "        if self.sparse:\n",
    "            warnings.warn(\"Sparse implementation only returns hard labels\")\n",
    "            return self.get_answers()\n",
    "        return self.T\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
